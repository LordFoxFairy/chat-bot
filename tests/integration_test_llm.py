import asyncio
import os
import sys
from typing import List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° pythonpath
sys.path.append(os.getcwd())

from dotenv import load_dotenv
from langchain_core.messages import SystemMessage

from backend.adapters.llm.langchain_llm_adapter import LangChainLLMAdapter
from backend.core.models import TextData

async def test_llm_adapter():
    """æµ‹è¯• LangChainLLMAdapter"""
    print("\n" + "="*50)
    print("å¼€å§‹æµ‹è¯• LangChainLLMAdapter")
    print("="*50)

    # 1. ç¯å¢ƒå˜é‡æ£€æŸ¥
    load_dotenv()
    api_key = os.getenv("API_KEY")
    if not api_key:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ° API_KEY ç¯å¢ƒå˜é‡")
        return
    print(f"âœ… æˆåŠŸè¯»å– API_KEY (é•¿åº¦: {len(api_key)})")

    # 2. æ¨¡æ‹Ÿé…ç½®
    config = {
        "model_name": "anthropic/claude-3.5-sonnet", # ä½¿ç”¨ config.yaml ä¸­çš„æ¨¡å‹
        "api_key_env_var": "API_KEY",
        "base_url": "https://openrouter.ai/api/v1",
        "temperature": 0.7,
        "max_tokens": 1024
    }

    # åˆå§‹åŒ– adapter
    try:
        adapter = LangChainLLMAdapter(module_id="test_llm", config=config)
        # æ‰‹åŠ¨è®¾ç½® system promptï¼Œæ¨¡æ‹Ÿ application context
        adapter.system_prompt = "ä½ æ˜¯ä¸€ä¸ªæ•°å­¦åŠ©æ‰‹ï¼Œè¯·åªå›ç­”æ•°å­—ç»“æœï¼Œä¸è¦å¸¦å…¶ä»–æ–‡å­—ã€‚"
        print(f"âœ… Adapter initialized with model: {config['model_name']}")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return

    # åˆå§‹åŒ–å†…éƒ¨ LLM
    try:
        await adapter._setup_impl()
        print("âœ… LLM model client initialized")
    except Exception as e:
        print(f"âŒ LLM å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        return

    # 3. æ„é€ è¾“å…¥
    question = "1+1ç­‰äºå‡ ï¼Ÿ"
    input_data = TextData(text=question)
    session_id = "test_session_001"

    print(f"\nğŸ“¤ å‘é€é—®é¢˜: {question}")

    # 4. æ¥æ”¶æµå¼å›å¤
    print("ğŸ“¥ æ¥æ”¶å›å¤: ", end="", flush=True)
    full_response = ""
    received_chunks = 0

    try:
        async for chunk in adapter.chat_stream(input_data, session_id):
            if chunk.is_final:
                continue

            content = chunk.text
            if content:
                print(content, end="", flush=True)
                full_response += content
                received_chunks += 1

        print("\n")

        # 5. éªŒè¯ç»“æœ
        print("-" * 30)
        expected_answer = "2"
        if expected_answer in full_response:
            print(f"âœ… æµ‹è¯•é€šè¿‡: å›å¤ä¸­åŒ…å« '{expected_answer}'")
            print(f"   å®Œæ•´å›å¤: {full_response}")
            print(f"   æ”¶åˆ° chunks: {received_chunks}")
        else:
            print(f"âŒ æµ‹è¯•å¤±è´¥: å›å¤ä¸­æœªæ‰¾åˆ° '{expected_answer}'")
            print(f"   å®é™…å›å¤: {full_response}")

    except Exception as e:
        print(f"\nâŒ å¤„ç†è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        # æ¸…ç†
        await adapter._close_impl()

if __name__ == "__main__":
    asyncio.run(test_llm_adapter())
